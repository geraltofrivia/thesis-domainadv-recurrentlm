{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Lib imports\n",
    "import collections\n",
    "import html\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
    "\n",
    "# FastAI Imports\n",
    "from fastai import text, core, lm_rnn\n",
    "\n",
    "# Torch imports\n",
    "import torch.nn as nn\n",
    "import torch.tensor as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Mytorch imports\n",
    "from mytorch import loops as mtlp\n",
    "from mytorch.utils.goodies import *\n",
    "from mytorch import lriters as mtlr\n",
    "\n",
    "import utils\n",
    "from options import Phase2 as params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fec2951d2f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DEBUG = True\n",
    "TRIM = False\n",
    "\n",
    "# Path fields\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "WIKI_DATA_PATH = Path('raw/wikitext/wikitext-103/')\n",
    "WIKI_DATA_PATH.mkdir(exist_ok=True)\n",
    "IMDB_DATA_PATH = Path('raw/imdb/aclImdb/')\n",
    "IMDB_DATA_PATH.mkdir(exist_ok=True)\n",
    "PATH = Path('resources/proc/imdb')\n",
    "DATA_PROC_PATH = PATH / 'data'\n",
    "DATA_LM_PATH = PATH / 'datalm'\n",
    "\n",
    "LM_PATH = Path('resources/models')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "PRE_PATH = LM_PATH / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "WIKI_CLASSES = ['wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DomainAgnosticSampler:\n",
    "    \"\"\" Sample data for language model training from two different domains in one batch. \"\"\"\n",
    "\n",
    "    def __init__(self, data_fn, data_a, data_b):\n",
    "        \"\"\"\n",
    "            Here, data_fn would be something like\n",
    "                `partial(text.LanguageModelLoader, bs=bs, bptt=bptt)`\n",
    "            And data_a/b would be something like\n",
    "                `{'train': np.concatenate(trn_lm), 'valid': np.concatenate(val_lm)}['train']`\n",
    "            data_fn (fastai's language model loader) flattens y and returns x of seqlen, batchsize\n",
    "        \"\"\"\n",
    "        self.args = {'data_fn': data_fn, 'data_a': data_a, 'data_b': data_b}\n",
    "        self.reset(**self.args)\n",
    "\n",
    "    def reset(self, data_fn, data_a, data_b):\n",
    "        self.itera = iter(data_fn(data_a))\n",
    "        self.iterb = iter(data_fn(data_b))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        x_a, y_a = self.itera.__next__()\n",
    "        x_b, y_b = self.iterb.__next__()\n",
    "        return self._combine_batch_(x_a, x_b, y_a, y_b)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.args['data_fn'](self.args['data_a'])),\n",
    "                   len(self.args['data_fn'](self.args['data_b'])))\n",
    "\n",
    "    @staticmethod\n",
    "    def _combine_batch_(x_a, x_b, y_a, y_b):\n",
    "        \"\"\"\n",
    "            :param x_a is a np.arr looks like seqlen, batchsize\n",
    "            :param y_a is a corresponding np.arr (one word ahead than x_a) which is a flattened x_a.shape mat\n",
    "             Same for x_b, y_b\n",
    "\n",
    "             Returns x, y, y_dom in similar shapes as input\n",
    "        \"\"\"\n",
    "\n",
    "        # Get them to interpretable shapes\n",
    "        y_a = y_a.reshape(x_a.shape).transpose(1, 0)\n",
    "        y_b = y_b.reshape(x_b.shape).transpose(1, 0)\n",
    "        x_a = x_a.transpose(1, 0)\n",
    "        x_b = x_b.transpose(1, 0)\n",
    "\n",
    "        b_bs, b_sl = x_a.shape[0], min(x_a.shape[1], x_b.shape[1])\n",
    "\n",
    "        # Concatenate to make an x and y\n",
    "        x = np.concatenate((x_a[:, :b_sl], x_b[:, :b_sl]))\n",
    "        y = np.concatenate((y_a[:, :b_sl], y_b[:, :b_sl]))\n",
    "\n",
    "        # Shuffle and remember shuffle index to make y labels for domain agnostic training\n",
    "        intrp = np.arange(b_bs * 2)\n",
    "        np.random.shuffle(intrp)\n",
    "        y_dom = (intrp >= b_bs) * 1\n",
    "        x = x[intrp]\n",
    "        y = y[intrp]\n",
    "\n",
    "        x = x.transpose(1, 0)\n",
    "        y = y.transpose(1, 0).reshape(np.prod(y.shape))\n",
    "\n",
    "        return x, y, y_dom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load **WIKI** data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859955 1841\n"
     ]
    }
   ],
   "source": [
    "def is_valid_sent(x):\n",
    "    x = x.strip()\n",
    "    if len(x) == 0: return False\n",
    "    if x[0] == '=' and x[-1] == '=': return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def wiki_get_texts_org(path):\n",
    "    texts = []\n",
    "    for idx, label in enumerate(WIKI_CLASSES):\n",
    "        with open(path / label, encoding='utf-8') as f:\n",
    "            texts.append([sent.strip() for sent in f.readlines() if is_valid_sent(sent)])\n",
    "    return tuple(texts)\n",
    "\n",
    "\n",
    "wiki_trn_texts, wiki_val_texts, wiki_tst_texts = wiki_get_texts_org(WIKI_DATA_PATH)\n",
    "print(len(wiki_trn_texts), len(wiki_val_texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, trim data to only keep top 1000 things from both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRIM:\n",
    "    wiki_trn_texts, wiki_val_texts, wiki_tst_texts = wiki_trn_texts[:1000], wiki_val_texts[:1000], wiki_tst_texts[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle data & Make dummy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(wiki_trn_texts)\n",
    "np.random.shuffle(wiki_val_texts)\n",
    "np.random.shuffle(wiki_tst_texts)\n",
    "\n",
    "wiki_trn_labels = [0 for _ in wiki_trn_texts]\n",
    "wiki_val_labels = [0 for _ in wiki_val_texts]\n",
    "wiki_tst_labels = [0 for _ in wiki_val_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe magic & Fixing up text & Splitting into train and Val\n",
    "\n",
    "_note_: We read generously from the valid data. Should do something about it!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     4,
     12,
     22,
     30
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777582 86399\n",
      "Trn: (777582, 777582), Val: (86399, 86399) \n"
     ]
    }
   ],
   "source": [
    "chunksize = 24000\n",
    "re1 = re.compile(r'  +')\n",
    "col_names = ['labels', 'text']\n",
    "\n",
    "def _fixup_(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "def _get_texts_(df, n_lbls=1):\n",
    "    labels = df.iloc[:, range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls + 1, len(df.columns)): texts += f' {FLD} {i - n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(_fixup_).values)\n",
    "\n",
    "    tok = text.Tokenizer().proc_all_mp(core.partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "\n",
    "def _simple_apply_fixup_(df):\n",
    "    labels = [0] * df.shape[0]\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df.text\n",
    "    texts = list(texts.apply(_fixup_).values)\n",
    "    tok = text.Tokenizer().proc_all_mp(core.partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = _get_texts_(r, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels\n",
    "\n",
    "\n",
    "# trn_texts, val_texts = sklearn.model_selection.train_test_split(\n",
    "#         np.concatenate([trn_texts, val_texts]), test_size=0.1)\n",
    "\n",
    "# if DEBUG:\n",
    "#     print(len(trn_texts), len(val_texts))\n",
    "\n",
    "# # df_trn = pd.DataFrame({'text': trn_texts, 'labels': [0] * len(trn_texts)}, columns=col_names)\n",
    "# # df_val = pd.DataFrame({'text': val_texts, 'labels': [0] * len(val_texts)}, columns=col_names)\n",
    "\n",
    "# trn_tok, trn_labels = _simple_apply_fixup_(df_trn)\n",
    "# val_tok, val_labels = _simple_apply_fixup_(df_val)\n",
    "\n",
    "# if DEBUG:\n",
    "#     print(f\"Trn: {len(trn_tok), len(trn_labels)}, Val: {len(val_tok), len(val_labels)} \")\n",
    "\n",
    "wiki_trn_texts, wiki_val_texts = sklearn.model_selection.train_test_split(\n",
    "    np.concatenate([wiki_trn_texts, wiki_val_texts, wiki_tst_texts]), test_size=0.1)\n",
    "\n",
    "if DEBUG:\n",
    "    print(len(wiki_trn_texts), len(wiki_val_texts))\n",
    "\n",
    "wiki_df_trn = pd.DataFrame({'text': wiki_trn_texts, 'labels': [0] * len(wiki_trn_texts)}, columns=col_names)\n",
    "wiki_df_val = pd.DataFrame({'text': wiki_val_texts, 'labels': [0] * len(wiki_val_texts)}, columns=col_names)\n",
    "\n",
    "wiki_trn_tok, wiki_trn_labels = _simple_apply_fixup_(wiki_df_trn)\n",
    "wiki_val_tok, wiki_val_labels = _simple_apply_fixup_(wiki_df_val)\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"Trn: {len(wiki_trn_tok), len(wiki_trn_labels)}, Val: {len(wiki_val_tok), len(wiki_val_labels)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we make vocabulary, select 60k most freq words \n",
    " \n",
    " _note_: we do this looking only at imdb, and ignore wiki here **NOT ANY MORE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITOS: 60002, STOI: 60002\n",
      "ITOS: 60002, STOI: 265350\n"
     ]
    }
   ],
   "source": [
    "freq = Counter(p for o in wiki_trn_tok for p in o)\n",
    "# freq.most_common(25)\n",
    "max_vocab = params.max_vocab_task\n",
    "min_freq = 2\n",
    "\n",
    "itos = [o for o, c in freq.most_common(max_vocab) if c > min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "stoi = collections.defaultdict(lambda: 0, {v: k for k, v in enumerate(itos)})\n",
    "\n",
    "# # This block here also takes words from wiki while making vocabulary.\n",
    "# wiki_freq = Counter(p for o in wiki_trn_tok for p in o)\n",
    "# # freq.most_common(25)\n",
    "# wiki_max_vocab = params.max_vocab_wiki\n",
    "# wiki_most_common_words = wiki_freq.sorted()\n",
    "\n",
    "# for word, count in wiki_most_common_words:\n",
    "#     if len(itos) >= max_vocab + wiki_max_vocab:\n",
    "#         break\n",
    "\n",
    "#     if word not in stoi.keys():\n",
    "#         itos.append(word)\n",
    "#         stoi[word] = len(stoi)\n",
    "\n",
    "\n",
    "vs = len(itos)\n",
    "# trn_lm = np.array([[stoi[o] for o in p] for p in trn_tok])\n",
    "# val_lm = np.array([[stoi[o] for o in p] for p in val_tok])\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"ITOS: {len(itos)}, STOI: {len(stoi)}\")\n",
    "\n",
    "wiki_trn_lm = np.array([[stoi[o] for o in p] for p in wiki_trn_tok])\n",
    "wiki_val_lm = np.array([[stoi[o] for o in p] for p in wiki_val_tok])\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"ITOS: {len(itos)}, STOI: {len(stoi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pull pretrained models from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz, nh, nl = 400, 1150, 3\n",
    "# PRE_PATH = PATH / 'models' / 'wt103'\n",
    "# PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)\n",
    "enc_wgts = core.to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "itos2 = pickle.load((PRE_PATH / 'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda: -1, {v: k for k, v in enumerate(itos2)})\n",
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i, w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r >= 0 else row_m\n",
    "\n",
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))\n",
    "wgts_enc = {'.'.join(k.split('.')[1:]): val\n",
    "            for k, val in wgts.items() if k[0] == '0'}\n",
    "wgts_dec = {'.'.join(k.split('.')[1:]): val\n",
    "            for k, val in wgts.items() if k[0] == '1'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0,
     9,
     23,
     41
    ]
   },
   "outputs": [],
   "source": [
    "class CustomEncoder(lm_rnn.RNN_Encoder):\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n",
    "\n",
    "\n",
    "class CustomDecoder(text.LinearDecoder):\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([self.decoder, self.dropout])\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.dropout(outputs[-1])\n",
    "        decoded = self.decoder(output.view(output.size(0) * output.size(1), output.size(2)))\n",
    "        result = decoded.view(-1, decoded.size(1))\n",
    "        return result, raw_outputs, outputs\n",
    "\n",
    "\n",
    "class CustomLinear(lm_rnn.PoolingLinearClassifier):\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = outputs[-1]\n",
    "        sl,bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        mxpool = self.pool(output, bs, True)\n",
    "        x = torch.cat([output[-1], mxpool, avgpool], 1)\n",
    "        for i, l in enumerate(self.layers):\n",
    "            l_x = l(x)\n",
    "            if i != len(self.layers) -1:\n",
    "                x = F.relu(l_x)\n",
    "            else:\n",
    "                x = torch.sigmoid(l_x)\n",
    "        return l_x, raw_outputs, outputs\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 _parameter_dict,\n",
    "                 _device,\n",
    "                 _wgts_e,\n",
    "                 _wgts_d,\n",
    "                 _encargs):\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "\n",
    "        self.encoder = CustomEncoder(**_encargs).to(self.device)\n",
    "        self.encoder.load_state_dict(_wgts_e)\n",
    "        \"\"\"\n",
    "            Explanation:\n",
    "                400*3 because input is [ h_T, maxpool, meanpool ]\n",
    "                0.4, 0.1 are drops at various layersLM_PATH\n",
    "        \"\"\"\n",
    "        self.linear_dec = CustomDecoder(\n",
    "            _encargs['ntoken'],\n",
    "            n_hid=400,\n",
    "            dropout=params.decoder_drops,\n",
    "            tie_encoder=self.encoder.encoder,\n",
    "            bias=False\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.linear_dec.load_state_dict(_wgts_d)\n",
    "        self.encoder.reset()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_enc = self.encoder(x)\n",
    "        return self.linear_dec(x_enc)\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        # layers = [x for x in self.encoder.layers]\n",
    "        # layers.append(torch.nn.ModuleList([x for x in self.linear_dec.layers]))\n",
    "        # layers.append(torch.nn.ModuleList([x for x in self.linear_dom.layers]))\n",
    "        # return torch.nn.ModuleList(layers)\n",
    "        return self.encoder.layers.extend(self.linear_dec.layers)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            pred = self.forward(x)\n",
    "            self.train()\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 70\n",
    "bs = params.bs\n",
    "opt_fn = partial(torch.optim.SGD)  #, betas=params.adam_betas)  # @TODO: find real optimizer, and params\n",
    "\n",
    "# Load the pre-trained model\n",
    "parameter_dict = {'itos2': itos2}\n",
    "dps = params.encoder_drops\n",
    "encargs = {'ntoken': new_w.shape[0],\n",
    "           'emb_sz': 400, 'n_hid': 1150,\n",
    "           'n_layers': 3, 'pad_token': 0,\n",
    "           'qrnn': False, 'dropouti': dps[0],\n",
    "           'wdrop': dps[2], 'dropoute': dps[3], 'dropouth': dps[4]}\n",
    "\n",
    "lm = LanguageModel(parameter_dict, device, _wgts_e=wgts_enc, _wgts_d=wgts_dec, _encargs=encargs)\n",
    "opt = make_opt(lm, opt_fn, lr=params.lr.init)\n",
    "loss_main_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make data iterators and LR iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(seq: list, model: LanguageModel):\n",
    "    _seq = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(-1)\n",
    "    return torch.argmax(model.predict(_seq)[0], dim=-1).detach().cpu().numpy()\n",
    "def tostr(seq, itos):\n",
    "    return ' '.join([itos[x] for x in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n xbos xfld 1 when he performed in london , formby would change his act , introducing himself as \" good evening , i \\' m formby fra \\' wigan ... i \\' ve not been in england long \" ; he slightly modified his stage persona , and he played \" the na√Øve boy trying to fit in with the sophisticated south \" . smart and boothroyd consider that \" the contrast between his northern accent and metropolitan bravado was humorous , and the more _unk_ and sophisticated his audience the more george exaggerated his provincial u_n \" .',\n",
       " '\\n xfld 1 the the was the the , he was have his name to and the as \" the \" \" \" \\' m a \" \" \" \\' \" \\' m got been so a . before . he would changed his version name to and then wore the the man \" \" to get the with the stage , american . he \\'s his were this he the real between the performance accent and his accent \" a \" and he audience he he more he performance was more he he . voice accent \" . the')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = get_predictions(wiki_trn_lm[10], lm)\n",
    "_x, _ypred = tostr(wiki_trn_lm[10], itos), tostr(ypred, itos) \n",
    "_x, _ypred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
